{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Business Improvement Areas in Toronto Using Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the city of Toronto, a Business Improvement Area (BIA) is an \"association of commercial property owners and tenants within a defined area who work in partnership with the City to create thriving, competitive, and safe business areas that attract shoppers, diners, tourists, and new businesses\". The goal of this project is to leverage data to aid commercial property owners and tenants with the process of defining new BIA's based on the location and tpy eof businesses in the city.\n",
    "\n",
    "For this project, I used three datasets from the City of Toronto, implemented a Machine Learning algorithm from a research paper, and used the Foursquare Places API to take a detailed view of the location and types of businesses in Toronto. I looked at clusters of venues outside existing BIA's, compared them to venues inside existing BIA's, and found some interesting patterns that might help with the development of new BIA's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  # fill cell width to screen\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from concurrent import futures  # parallel processes\n",
    "\n",
    "from scipy.spatial import distance  # distance matrix computations\n",
    "from scipy.cluster import hierarchy  # hierarchical clustering\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler  # used to normalize data\n",
    "from sklearn import svm\n",
    "\n",
    "import networkx as nx  # plot graphs\n",
    "\n",
    "import requests  # handle requests\n",
    "import json\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pylab  # plotting dendograms\n",
    "import seaborn as sns  # plotting in general\n",
    "import branca  # add legend to folium map\n",
    "import folium  # map rendering library\n",
    "\n",
    "import geopandas  # DataFrame to GeoJSON\n",
    "import pyproj    \n",
    "import shapely\n",
    "import shapely.ops as ops\n",
    "from shapely.geometry import Point, MultiPoint, Polygon, MultiPolygon  # create shapely geometry objects\n",
    "from functools import partial\n",
    "from haversine import haversine_vector  # calculate distance between coordinates in lat, long\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHOME'] = r'C:\\Users\\diego\\Anaconda3'\n",
    "os.environ['PYTHONPATH'] = r'C:\\Users\\diego\\Anaconda3\\Lib\\site-packages'\n",
    "os.environ['R_HOME'] = r'C:\\Users\\diego\\Anaconda3\\Lib\\R'\n",
    "os.environ['R_USER'] = r'C:\\Users\\diego\\Anaconda3\\Lib\\site-packages\\rpy2'\n",
    "\n",
    "import rpy2.rinterface\n",
    "#import rpy2.robjects as ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o test\n",
    "\n",
    "test <- 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Vehicle and Pedestrian Volumes from the Open Data Toronto Catalogue\n",
    "The dataset contains the 8 peak hour vehicle and pedestrian volume counts collected at intersections where there are traffic signals. The data was collected between the hours of 7:30 a.m. and 6:00 p.m. It is worth noting that for each intersection, the data was collected on a single day is not an average of multiple days. This is a limitation of the data, but it is still useful since I will be looking at the pedestrian volume of neighbourhoods by grouping intersections. Therefore, they will be averaged and any discrepancy in data collection will be minimized. Furthermore, I am interested in the busiest neighbourhoods and, on a typical day, there will be a vast difference in pedestrian volume between the busiest neighbourhoods and the less busy ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset metadata by passing package_id to the package_search endpoint\n",
    "# For example, to retrieve the metadata for this dataset:\n",
    "url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca/api/3/action/package_show\"\n",
    "payload = {\"id\": \"ae4e10a2-9eaf-4da4-83fb-f3731a30c124\"}\n",
    "response = requests.get(url, params=payload).json()\n",
    "traffic_df = pd.read_excel(response[\"result\"]['resources'][0]['url'])\n",
    "print('Number of rows read in:', traffic_df.shape[0], '\\n')\n",
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the columns of the table and see if there are any missing values. Non-missing values will be coloured black, while missing values will be white. Also, check for duplicate intersections using their latitude and longitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(traffic_df.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique latitude and longitude values:', len(traffic_df.groupby(['Latitude', 'Longitude']).size()))\n",
    "print('Number of rows in the table:', traffic_df.shape[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values in two of the columns that define an intersection. However, I am only interested in the geographical location of the intersections, as well as their pedestrian and vehicle volumes so these two columns can be removed. Additionally, the count and activation dates are not needed for the purpose of this project so they will be removed as well. Finally, the vehicle volume gets removed given that in the busiest neighbourhoods of the city (i.e. downtown) most people are either driving through or driving to work and not driving to a venue in specific. The remaining columns are renamed to make the table easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_rename = {\n",
    "    'TCS #': 'Id',\n",
    "    'Side 1 Route': 'Side1',\n",
    "    '8 Peak Hr Pedestrian Volume': 'PedestrianVolume'}\n",
    "traffic_df.drop(['Midblock Route', 'Side 2 Route', '8 Peak Hr Vehicle Volume', 'Activation Date', 'Count Date'], axis=1, inplace=True)\n",
    "traffic_df.rename(columns=col_rename, inplace=True)\n",
    "\n",
    "# add column with the intersection name\n",
    "intersections = traffic_df.loc[:, 'Main'] + ' & ' + traffic_df.loc[:, 'Side1']\n",
    "traffic_df.loc[:, 'Intersection'] = intersections\n",
    "\n",
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the descriptive statistics of the table to see if any further cleaning is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is at least one intersection where the pedestrian volume is zero. We don't want to consider intersections where there is no pedestrian traffic since they're likely to be highway ramps and the likelihood of a business being at these intersections is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_rows = traffic_df.shape[0]\n",
    "traffic_df.drop(traffic_df[traffic_df['PedestrianVolume'] == 0].index, inplace=True)\n",
    "traffic_df.reset_index(inplace=True, drop=True)\n",
    "new_rows = traffic_df.shape[0]\n",
    "print('Number of rows with 0 pedestrian traffic that were dropped:', orig_rows - new_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows a scatter plot of the latitudes and longitudes in the data. This should have roughly the shape of the city, but we see that there is an outlier to the right of -74 degrees of longitude. Let's take a look at this entry and see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.plot(kind='scatter', x='Longitude', y='Latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.loc[traffic_df['Longitude'] > -74]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you search the intersection in Google Maps, you will see that it is in the downtown area. The longitude value was recorded incorrectly and its correct value is -79.3874532, so it needs to be corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.loc[traffic_df['Id'] == 2377, 'Longitude'] = -79.3874532\n",
    "traffic_df.loc[traffic_df['Id'] == 2377]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "traffic_df.plot(kind='scatter', x='Longitude', y='Latitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the plot now resembles the shape of the city. Below are histograms of the numerical data in the table and there doesn't seem to be any large outliers so no more cleaning is needed for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.hist(figsize=(8,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Neighbourhood Profiles from the Open Data Toronto Catalogue\n",
    "Neighbourhoods in Toronto refer to the city's 140 social planning neighbourhoods. These social planning neighbourhoods help government and community organizations with local planning by providing socio-economic data at a meaningful geographic data. We are interested in the boundaries of these neighbourhoods, which remain consisten over time. The data is sourced by Statistics Canada. To read more about neighbourhood profiles, visit its Open Data Toronto page at https://open.toronto.ca/dataset/neighbourhood-profiles/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbh_gdf = geopandas.read_file('https://ckan0.cf.opendata.inter.prod-toronto.ca/download_resource/a083c865-6d60-4d1d-b6c6-b0c8a85f9c15?format=geojson&projection=4326')\n",
    "print('Number of rows read in:', nbh_gdf.shape[0])\n",
    "nbh_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm only interested in the location and shape (geometry) of each neighbourhood, so all other columns are removed and the remaining ones renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbh_gdf.drop(labels=['_id', 'AREA_ID', 'AREA_ATTR_ID', 'PARENT_AREA_ID', 'AREA_SHORT_CODE',\n",
    "                              'AREA_LONG_CODE', 'AREA_DESC', 'X', 'Y', 'OBJECTID',\n",
    "                              'Shape__Area', 'Shape__Length'], axis=1, inplace=True)\n",
    "nbh_gdf.rename(columns={'AREA_NAME': 'Neighbourhood',\n",
    "                        'LATITUDE': 'Latitude',\n",
    "                        'LONGITUDE': 'Longitude'}, inplace=True)\n",
    "\n",
    "nbh_clean = nbh_gdf['Neighbourhood'].str.replace(r\"\\(.*\\)\",\"\").str.strip().to_list()\n",
    "\n",
    "nbh_gdf.loc[:, 'Neighbourhood'] = nbh_clean\n",
    "\n",
    "nbh_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any missing data, and see if the column types are appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(nbh_gdf.isnull(), cbar=False)\n",
    "\n",
    "print(nbh_gdf.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing data, and the column types make sense. Now check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Are there any duplicate neighbourhoods:', nbh_gdf.shape[0] != len(nbh_gdf.groupby(['Latitude', 'Longitude']).size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to have the area of each neighbourhood, so I will transform the projection of the polygon to an _equal area projection_ in order to get their area in squared kilometers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbh_proj = nbh_gdf.loc[:, ('Neighbourhood', 'geometry')]\n",
    "with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        nbh_proj = nbh_proj.to_crs({'proj': 'cea'})\n",
    "nbh_proj['Area'] = nbh_proj.loc[:, 'geometry'].area / 10**6\n",
    "nbh_proj.drop('geometry', axis=1, inplace=True)\n",
    "nbh_gdf = nbh_gdf.merge(nbh_proj.set_index('Neighbourhood'), on='Neighbourhood')\n",
    "nbh_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Neighbourhoods and Intersections\n",
    "It is helpful to visualize the neighbourhoods and the intersections together, so I created a map that overlays both datasets on top of a map of Toronto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define centre latitude and longitude for the city\n",
    "centre_lat = 43.7064\n",
    "centre_lng = -79.3986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitZoomBounds(m, latitudes, longitudes, offset=0):\n",
    "    # Function to get the coordinates for the bounding box that defines the level of zoom in a\n",
    "    # folium map.\n",
    "    #\n",
    "    # Args:\n",
    "    #   m: folium Map object\n",
    "    #   latitudes: array of latitude values.\n",
    "    #   longitudes: array of longitude values.\n",
    "    #   offset: proportion of the maximum latitude and longitude used to offset the bounding box\n",
    "    #           corners. Positive number to contract the box, negative number to dilate the box.\n",
    "    #\n",
    "    # Returns: None\n",
    "    \n",
    "    min_lat, max_lat = latitudes.min(), latitudes.max()\n",
    "    min_lng, max_lng = longitudes.min(), longitudes.max()\n",
    "    max_lat_pct = max_lat * offset\n",
    "    max_lng_pct = max_lng * -offset\n",
    "    sw = [min_lat + max_lat_pct, min_lng + max_lng_pct]\n",
    "    ne = [max_lat - max_lat_pct, max_lng - max_lng_pct]\n",
    "    \n",
    "    m.fit_bounds([sw, ne])\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def CreateMap(centre_lat, centre_lng, **kwargs):\n",
    "    # Function to create a Folium Map Object.\n",
    "    #\n",
    "    # Args:\n",
    "    #   centre_lat: latitude value of the centre of the map\n",
    "    #   centre_lng: longitude value of the centre of the map\n",
    "    #   **kwargs: addtional keyword arguments passed to folium class folium.folium.Map\n",
    "    #\n",
    "    # Returns:\n",
    "    #   m: a Folium Map Object\n",
    "    \n",
    "    m = folium.Map(\n",
    "        location=[centre_lat, centre_lng],\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return m\n",
    "\n",
    "\n",
    "def AddCircleMarkers(m, latitudes, longitudes, labels, **kwargs):\n",
    "    # Function to add circle markers to the folium Map m.\n",
    "    #\n",
    "    # Args:\n",
    "    #   m: folium Map object\n",
    "    #   latitudes: list of latitude values to plot\n",
    "    #   longitudes: list of longitude values to plot; must be same length as latitudes\n",
    "    #   labels: labels for the markers; must be same length as latitudes\n",
    "    #   **kwargs: additional keyword arguments passed to the folium class\n",
    "    #             folium.vector.layers.CircleMarker\n",
    "    #\n",
    "    # Returns: None\n",
    "    \n",
    "    for lat, lng, label in zip(latitudes,\n",
    "                               longitudes,\n",
    "                               labels):\n",
    "\n",
    "        folium.Circle(\n",
    "            [lat, lng],\n",
    "            popup=label,\n",
    "            **kwargs\n",
    "        ).add_to(m)\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "def AddPolygonLayer(m, gdf, return_layer=False, **kwargs):\n",
    "    # Function to add a GeoJson layer to a folium Map object.\n",
    "    #\n",
    "    # Args:\n",
    "    #   m: folium Map object\n",
    "    #   gdf: GeoDataFrame with geometry data for the GeoJson layer\n",
    "    #   return_layer: True if the GeoJson layer is to be returned, otherwise do nothing\n",
    "    #   **kwargs: additional keyword arguments passed to the folium class folium.features.GeoJson\n",
    "    #\n",
    "    # Returns:\n",
    "    #    geo_layer, if return_layer = True\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        geo_layer = folium.GeoJson(gdf, **kwargs).add_to(m)\n",
    "\n",
    "    if return_layer:\n",
    "        return geo_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CreateMap(centre_lat, centre_lng, zoom_start=11, scrollWheelZoom=False)\n",
    "\n",
    "AddPolygonLayer(m, nbh_gdf)\n",
    "\n",
    "AddCircleMarkers(m, traffic_df['Latitude'], traffic_df['Longitude'], traffic_df['Intersection'],\n",
    "                 radius=3, color='black', weight=3)\n",
    "                 \n",
    "FitZoomBounds(m, traffic_df['Latitude'], traffic_df['Longitude'], 0.0002)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the boundaries of each neighbourhood are well defined, and they fit well within the boundaries of the city. The next step is to associate each intersection with the neighbourhood they are in, and explore the busiest neighbourhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Intersections to their Neighbourhoods\n",
    "The goal here is to assign each intersection to a neighbourhood and then we can see which neighbourhoods are the busiest in terms of pedestrian volume. In order to check that an intersection is inside a neighbourhood, we will use the _shapely_ library which allows us to check if a point is inside a geometry object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def GetNbhForPoints(nbh_names, nbh_polygons, point_ids, points, point_id_name='Id', col_name='Neighbourhood'):\n",
    "    # Function to get the neighbourhoods that contain each point.\n",
    "    #\n",
    "    # Args:\n",
    "    #   nbh_names: Series of neighbourhood names\n",
    "    #   nbh_polygons: geopandas.GeoSeries of shapely.geometry.Polygon objects that correspond to each \n",
    "    #                 neighbourhood in nbh_names\n",
    "    #   point_ids: list of IDs for each point in points\n",
    "    #   points: geopandas.GeoSeries of shapely.geometry.Point objects\n",
    "    #   point_id_name: the name of the ID column in the DataFrame that will be returned\n",
    "    #   col_name: string, the name of the resulting column\n",
    "    #\n",
    "    # Returns:\n",
    "    #    point_nbh_df: DataFrame, the first column is the unique ID of the Point, and the second\n",
    "    #                  column is the neighbourhood that contains the Point\n",
    "    \n",
    "    point_nbh_names = []\n",
    "    for point in points:\n",
    "        # list of booleans, True if polygon fully contains the point, False otherwise\n",
    "        t = nbh_polygons.contains(point)\n",
    "\n",
    "        # index of the list that contains the value True\n",
    "        idx = np.where(t)\n",
    "\n",
    "       # assign the area name if it was a match\n",
    "        if len(idx[0]) == 1:\n",
    "            nbh_name = nbh_names[idx[0][0]]\n",
    "        elif len(idx[0]) > 1:\n",
    "            # there are overlapping polygons that contain the point, so we'll choose the smallest polygon\n",
    "            overlap_poly = nbh_polygons[idx[0]]\n",
    "            nbh_name = nbh_names[overlap_poly.index.values[np.argmin(overlap_poly.area)]]\n",
    "        else:\n",
    "            nbh_name = 'Undefined'\n",
    "\n",
    "        point_nbh_names.append(nbh_name)\n",
    "        \n",
    "    point_nbh_df = pd.DataFrame(np.array([point_ids, point_nbh_names]).T,\n",
    "                               columns=[point_id_name, col_name])\n",
    "    \n",
    "    return point_nbh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a geometry column for the intersections\n",
    "points_list = [Point(lng, lat) for lng, lat in zip(traffic_df['Longitude'], traffic_df['Latitude'])]\n",
    "traffic_df['geometry'] = points_list\n",
    "\n",
    "# assign the neighbourhoods\n",
    "point_nbh_df = GetNbhForPoints(nbh_gdf['Neighbourhood'], nbh_gdf['geometry'], \n",
    "                               traffic_df.index.values, traffic_df['geometry'])\n",
    "\n",
    "traffic_df = traffic_df.merge(point_nbh_df, how='inner', left_index=True, right_index=True)\n",
    "traffic_df.drop('Id_y', axis=1, inplace=True)\n",
    "traffic_df.rename(columns={'Id_x': 'Id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function GetNbhForPoints, intersections are assigned a neighbourhood with the name 'Undefined' if the polygon didn't fully contain the point. Below is a plot of the intersections with no neighbourhood assigned to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.loc[traffic_df['Neighbourhood'] == 'Undefined']['Neighbourhood'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undef_nbh = traffic_df.loc[traffic_df['Neighbourhood'] == 'Undefined']\n",
    "\n",
    "m = CreateMap(centre_lat, centre_lng, zoom_start=10, scrollWheelZoom=False)\n",
    "\n",
    "AddPolygonLayer(m, nbh_gdf)\n",
    "\n",
    "AddCircleMarkers(m, undef_nbh['Latitude'], undef_nbh['Longitude'],\n",
    "                 undef_nbh['Intersection'], radius=3, color='black', weight=3)\n",
    "\n",
    "FitZoomBounds(m, undef_nbh['Latitude'], undef_nbh['Longitude'], 0.0002)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the map above, we see that the intersections with no assigned neighbourhood are located at the borders of their respective neighbourhood. In order to assign them their correct neighbourhood, we will determine the closest neighbourhood to each intersection and use that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceUndefinedPoly(points_df, poly_gdf, poly_col, max_dist=0):\n",
    "    # Function to find and assign the closest neighbourhood to points that have an undefined\n",
    "    # neighbourhood.\n",
    "    #\n",
    "    # Args:\n",
    "    #   points_df: DataFrame or geopandas.GeoDataFrame that contains points with neighbourhoods\n",
    "    #              assigned to them. Must have the column 'geometry' of shapely.Point objects. Must\n",
    "    #              have a column with the name specified by poly_col.\n",
    "    #   poly_gdf: geopandas.DataFrame that contains the polygons. Must have the column 'geometry' of \n",
    "    #             shapely.Polygon objects. Must have a column called poly_col.\n",
    "    #   poly_col: string, column name in points_df of the column with names of the polygons that\n",
    "    #             correspond to the polygons in poly_gdf\n",
    "    #   max_dist: float, maximum distance away from the polygon to be considered inside the polygon\n",
    "    #\n",
    "    # Effects:\n",
    "    #   Modifies the 'Neighbourhood' column of points_df.\n",
    "    #\n",
    "    # Returns: None\n",
    "    \n",
    "    assert max_dist >= 0, 'The value of max_dist must be >= 0'\n",
    "    assert poly_col in points_df.columns, 'There is no \"' + poly_col + '\" column in points_df'\n",
    "    assert poly_col in poly_gdf.columns, 'There is no \"' + poly_col + '\" column in poly_gdf'\n",
    "    \n",
    "    undef_nbh = points_df.loc[points_df[poly_col] == 'Undefined']\n",
    "    \n",
    "    if undef_nbh.shape[0] == 0:\n",
    "        warnings.warn(\"There are no 'Undefined' neighbourhoods. DataFrame will remain unchanged.\")\n",
    "        return\n",
    "    \n",
    "    for idx, point in zip(undef_nbh.index, undef_nbh['geometry']):\n",
    "        if max_dist == 0:\n",
    "            # find the closest Polygon to the point\n",
    "            closest_poly = min(poly_gdf['geometry'], key=point.distance)\n",
    "            \n",
    "        else:\n",
    "            d = poly_gdf.geometry.distance(point)\n",
    "            d = d[d <= max_dist]\n",
    "            \n",
    "            if len(d) == 0:\n",
    "                continue\n",
    "                \n",
    "            closest_poly = poly_gdf['geometry'][d.index[np.argmin(d)]]\n",
    "            \n",
    "        # get the area name for the closest Polygon\n",
    "        poly_name = poly_gdf.iloc[list(poly_gdf.geometry.values).index(closest_poly)][poly_col]   \n",
    "\n",
    "        points_df.at[idx, poly_col] = poly_name\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReplaceUndefinedPoly(traffic_df, nbh_gdf, 'Neighbourhood')\n",
    "\n",
    "# check that there are no 'Undefined' area names\n",
    "print(\"Number of 'Undefined' Neighbourhoods:\", \n",
    "      traffic_df.loc[traffic_df['Neighbourhood'] == 'Undefined'].shape[0], '\\n')\n",
    "    \n",
    "# check that the area names were assigned\n",
    "traffic_df.loc[traffic_df.index.isin(undef_nbh.index)][['Intersection', 'Neighbourhood']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every intersection belongs to a neighbourhood, so now we can determine which neighbourhoods are the busiest. We want to visualize the neighbourhoods based on their pedestrian volume in order to choose a region with the highest volume. This will allow us to choose an area of interest that we'll use for retrieving venues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Busiest Neighbourhoods in the City\n",
    "First, we need to get the average pedestrian volume in each neighbourhood and transform that table into a GeoDataFrame in order to create a Choropleth map. The Choropleth map will allow us to visualize the pedestrian volume in each neighbourhood by colouring the busiest neighbourhoods with a darker shade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbh_df = traffic_df.groupby('Neighbourhood')['PedestrianVolume'].mean().reset_index(name='AvgPedestrianVolume')\n",
    "nbh_df.loc[:, 'AvgPedestrianVolume'] = np.round(nbh_df.loc[:, 'AvgPedestrianVolume'], 2)\n",
    "\n",
    "# get the geometry of every neighbourhood\n",
    "nbh_df = nbh_df.merge(nbh_gdf[['Neighbourhood', 'geometry']].set_index('Neighbourhood'),\n",
    "                        how='inner', on='Neighbourhood')\n",
    "\n",
    "# convert to a GeoDataFrame so we can convert it to a GeoJSON for the Choropleth map\n",
    "nbh_plot_gdf = geopandas.GeoDataFrame(nbh_df)\n",
    "\n",
    "# add Coordinate Reference System for consistency\n",
    "nbh_plot_gdf.crs = {'init': 'epsg:4326'}\n",
    "\n",
    "nbh_plot_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DataFrame to GeoJSON file in order to use it in the Choropleth map\n",
    "nbh_plot_gdf.to_file('neighbourhood_pedestrian_vol.json', driver='GeoJSON')\n",
    "neighbourhood_geo = r'neighbourhood_pedestrian_vol.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a GeoJSON file with the neighbourhood and average pedestrian volume data, we can go ahead and create the Choropleth map. But first, let's look at the distribution of the average pedestrian volume across the neighbourhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = nbh_plot_gdf[['AvgPedestrianVolume']].plot(kind='hist', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we see that the distribution of the average pedestrian volume has a long tail. We want a linear distribution in order to get a better idea of which are the busiest neighbourhoods from the plot. In order to do so, we'll just take the log of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_avg_ped_vol = np.log(nbh_plot_gdf['AvgPedestrianVolume'])\n",
    "_ = plt.hist(log_avg_ped_vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the distribution looks better, let's create the Choropleth map using the log data. In the map below, you can use your mouse to hover over each neighbourhood and see its name along with the average pedestrian volume in that neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function by ox-baloo on https://github.com/python-visualization/folium/issues/1052\n",
    "def folium_del_legend(choropleth):\n",
    "    # remove choropleth legend\n",
    "    del_list = []\n",
    "    for child in choropleth._children:\n",
    "        if child.startswith('color_map'):\n",
    "            del_list.append(child)\n",
    "    for del_item in del_list:\n",
    "        choropleth._children.pop(del_item)\n",
    "    return choropleth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add the column to the table so it can be used in folium.Choropleth\n",
    "nbh_plot_gdf.loc[:, 'LogAvgPedestrianVolume'] = log_avg_ped_vol\n",
    "\n",
    "m = CreateMap(centre_lat, centre_lng, zoom_start=11, tiles='cartodbpositron', control_scale=True,\n",
    "             scrollWheelZoom=False)\n",
    "\n",
    "c = folium.Choropleth(\n",
    "    geo_data=neighbourhood_geo,\n",
    "    name='Average Pedestrian Volume in Toronto Neighbourhoods',\n",
    "    data=nbh_plot_gdf,\n",
    "    columns=['Neighbourhood', 'LogAvgPedestrianVolume'],\n",
    "    key_on='feature.properties.Neighbourhood',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.9,\n",
    "    line_opacity=0.2,\n",
    "    highlight=True\n",
    ").add_to(m)\n",
    "\n",
    "folium_del_legend(c).add_to(m)\n",
    "\n",
    "# add custom legend, had to use '.' in caption because the function deletes all whitespace characters\n",
    "# not possible to remove the numbers from the legend unfortunately\n",
    "cmap = plt.cm.get_cmap('YlOrRd')\n",
    "color_list = cmap(np.arange(cmap.N))\n",
    "leg = branca.colormap.LinearColormap(\n",
    "    colors=['#ffffcc','#fd863a', '#8d0026'],\n",
    "    caption='Less Busy ' + ''.join([char * 90 for char in '.']) +' Busiest')\n",
    "leg.add_to(m)\n",
    "\n",
    "# polygon map layer, with invisible colors\n",
    "style_function = lambda x: {'fillColor': '#00000000', 'color': '#00000000'}\n",
    "geo_layer = AddPolygonLayer(m, nbh_plot_gdf, return_layer=True, style_function=style_function)\n",
    "\n",
    "# add the tooltip for mouse hover\n",
    "folium.GeoJsonTooltip(['Neighbourhood', 'AvgPedestrianVolume']).add_to(geo_layer)\n",
    "geo_layer.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the map above, the busiest neighbourhoods are in the city centre and it's immediate surrounding areas. Now, we will choose the top 10 busiest neighbourhoods in order to define an area of the city where we would like to explore BIAs and venues. Below is a plot of the busiest neighbourhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busiest_nbh = nbh_plot_gdf.sort_values('AvgPedestrianVolume', ascending=False).head(10)\n",
    "\n",
    "m = CreateMap(centre_lat, centre_lng, zoom_start=11, scrollWheelZoom=False)\n",
    "\n",
    "geo_layer = AddPolygonLayer(m, busiest_nbh, return_layer=True)\n",
    "\n",
    "# add the tooltip for mouse hover\n",
    "folium.GeoJsonTooltip(['Neighbourhood', 'AvgPedestrianVolume']).add_to(geo_layer)\n",
    "geo_layer.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Venues in the Area of Interest\n",
    "Now that we have chosen a subset of neighbourhoods, the goal is to retrieve venue information in the area defined by the bounding box that contains all the neighbourhoods. To accomplish that, we will use the Foursquare Places API which allows us to get venue data for different types of businesses.\n",
    "<br><br>\n",
    "To get venue data for a specific area, we have to provide a set of coordinates and a numeric value that define a circle's centre and radius, respectively. The API then searches for venues inside the area defined by the circle. Unfortunately, you can only retrieve a maximum of 100 venues per API call; thus, it isn't possible to get all the venues in the city by providing a circle the size of our area of interest. To get past this limitation, there were two solutions I considered. The first one, was to keep making the same API call over and over again until the maximum amount allowed, with the hope that each time I would get a significantly different set of 100 venues. This didn't seem like a good idea to me since there was no way to guarantee that the venues would be equally distributed accross the city (e.g. there was the possibility that by chance none of the venues in a part of the city would be returned). The second solution, and the one that I implemented, involves creating a set of small, sometimes overlapping circles that cover the whole area of interest. Then you can make one API call in each circle and get the venues inside the small area defined by the cirle. The key is to define the circles small enough that it's plausible that 100 or less venues exist within that area. This way, we can get a much better distribution of venues in the city.\n",
    "<br><br>\n",
    "In this section, I will show how the circles were defined by creating a set of uniformly distributed points inside the bounding box and then implementing a _k_-Center Clustering algorithm to get circle centres given a fixed radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUnifRect(C, n):\n",
    "    # Function to get n uniformly distributed points bounded by the rectangle given by C.\n",
    "    #\n",
    "    # Args:\n",
    "    #   C: 2x4 matrix where each column specifies the (x, y) coordinates of the corners of the\n",
    "    #      rectangle in this order: bottom-left, top-left, top-right, bottom-right\n",
    "    #   n: int, number of points to fill the square with\n",
    "    #\n",
    "    # Returns:\n",
    "    #   unif_points: 2xn matrix where each column is the (x, y) coordinate of each point\n",
    "    \n",
    "    # get n point uniformly distributed in the unit square\n",
    "    x = np.random.uniform(0, 1, n)\n",
    "    y = np.random.uniform(0, 1, n)\n",
    "    U = np.vstack((x, y))\n",
    "    \n",
    "    # scale factors\n",
    "    s_x = (C[:1, 2] - C[:1, 0])[0]\n",
    "    s_y = (C[1:2, 1] - C[1:2, 0])[0]\n",
    "\n",
    "    # scaling matrix\n",
    "    S = np.vstack(([s_x, 0], [0, s_y]))\n",
    "    \n",
    "    # translation column\n",
    "    t = np.array([C[:, 0][0], C[:, 0][1]])[:, None]\n",
    "    \n",
    "    # new points scaled and translated\n",
    "    U_new = np.matmul(S, U) + t\n",
    "    \n",
    "    return U_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetBoundingBox(polygons):\n",
    "    # Function to get the bounding box of a group of polygons.\n",
    "    #\n",
    "    # Args:\n",
    "    #   polygons: pandas.Series of shapely.Polygon or shapely.MultiPolygon objects\n",
    "    #\n",
    "    # Returns:\n",
    "    #   sw, nw, ne, se: latitude and longitude values of the southwest, northwest, northeast, and \n",
    "    #                   southeast corners of the bounding box, respectively.\n",
    "    \n",
    "    # we want the minimum latitudes and longitudes across all polygons to create the box corners\n",
    "    min_lng = np.min(polygons.bounds['minx'])\n",
    "    max_lng = np.max(polygons.bounds['maxx'])\n",
    "    min_lat = np.min(polygons.bounds['miny'])\n",
    "    max_lat = np.max(polygons.bounds['maxy'])\n",
    "\n",
    "    # box corners\n",
    "    sw = np.array([min_lng, min_lat])\n",
    "    nw = np.array([min_lng, max_lat])\n",
    "    ne = np.array([max_lng, max_lat])\n",
    "    se = np.array([max_lng, min_lat])\n",
    "    \n",
    "    return sw, nw, ne, se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I define the bounding box for the neighbourhoods which is given by the min/max latitudes and longitudes. Furthermore, I modified the box to exclude points that fall in the water and those that fall on the Toronto Islands as well (there aren't many businesses in the islands). The next step is to choose BIA's that are on or within the bounding box, and extend this bounding box to fully contain the chosen BIA's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lat = 43.630243  # remove points on water and islands\n",
    "\n",
    "# get the bounding box of the neighbourhoods\n",
    "sw, nw, ne, se = GetBoundingBox(busiest_nbh.geometry)\n",
    "BBOX = np.array([sw, nw, ne, se]).T  # bounding box corners\n",
    "BBOX[1, 0] = min_lat\n",
    "BBOX[1, 3] = min_lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Business Improvement Areas in the Bounding Box\n",
    "First we'll load the dataset for BIA's from the Open Data Toronto catalogue. Then, we'll keep only BIA's on or inside the bounding box, and extend the bounding box to fully contain every one of them. The goal is to retrive venues that are inside the extended bounding box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bia_gdf = geopandas.read_file('https://ckan0.cf.opendata.inter.prod-toronto.ca/download_resource/d173e644-ace0-45e0-be43-8ba02fb116eb?format=geojson&projection=4326')\n",
    "print('Number of rows read in:', bia_gdf.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the columns that we don't need and rename the remaining ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bia_gdf.drop([\n",
    "    'AREA_ID', 'DATE_EFFECTIVE', 'AREA_ATTR_ID', 'PARENT_AREA_ID', 'AREA_SHORT_CODE', \n",
    "    'AREA_LONG_CODE', 'AREA_DESC', 'X', 'Y', 'OBJECTID', 'Shape__Area', 'Shape__Length'\n",
    "    ], axis=1, inplace=True\n",
    ")\n",
    "\n",
    "bia_gdf.rename(\n",
    "    columns={'_id': 'Id', 'AREA_NAME': 'BIA', 'LONGITUDE': 'Longitude', 'LATITUDE': 'Latitude'},\n",
    "    inplace=True\n",
    ")\n",
    "bia_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to only keep the BIAs that are are contained within or intersect the bounding box of the neighbourhoods of interest. Below is a map of the BIAs that meet this condition, as well as the bounding box of the neighbourhoods. If you hover over the BIA's you can look at their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bias = bia_gdf.shape[0]\n",
    "\n",
    "bbox = Polygon([BBOX[:, 0], BBOX[:, 1], BBOX[:, 2], BBOX[:, 3], BBOX[:, 0]])\n",
    "bia_list = []\n",
    "for bia, poly in zip(bia_gdf['BIA'], bia_gdf['geometry']):\n",
    "    if bbox.intersects(poly):\n",
    "        bia_list.append(bia)\n",
    "\n",
    "bia_gdf = bia_gdf.loc[np.in1d(bia_gdf['BIA'], bia_list)].reset_index(drop=True)\n",
    "print('Number of BIAs removed:', n_bias - bia_gdf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = CreateMap(centre_lat, centre_lng, zoom_start=11, scrollWheelZoom=False)\n",
    "\n",
    "folium.GeoJson(bbox).add_to(m)\n",
    "\n",
    "geo_layer = AddPolygonLayer(m, bia_gdf, return_layer=True)\n",
    "\n",
    "folium.GeoJsonTooltip(['BIA']).add_to(geo_layer)\n",
    "geo_layer.add_to(m)\n",
    "                \n",
    "FitZoomBounds(m, bia_gdf['Latitude'], bia_gdf['Longitude'], 1e-6)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see on the map above, there are a few BIA's that cross the boundary of the bounding box. Again, we'd like to extend this box to fully include all these BIA's to guarantee all the venues inside the BIA's will be captured. It is worth noting that we'll ignore 'The Waterfront' BIA since we decided to ignore points on the water and the islands. Below is a map of the extended bounding box with the BIA's fully contained in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bia_gdf = bia_gdf.loc[bia_gdf['BIA'] != 'The Waterfront'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corners of the box\n",
    "sw, nw, ne, se = GetBoundingBox(bia_gdf.geometry)\n",
    "\n",
    "boundary = Polygon([sw, nw, ne, se])\n",
    "\n",
    "# update the bounding box matrix\n",
    "BBOX = np.array([sw, nw, ne, se]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CreateMap(centre_lat, centre_lng, zoom_start=11, scrollWheelZoom=False)\n",
    "\n",
    "folium.GeoJson(boundary).add_to(m)\n",
    "\n",
    "geo_layer = AddPolygonLayer(m, bia_gdf, return_layer=True)\n",
    "\n",
    "folium.GeoJsonTooltip(['BIA']).add_to(geo_layer)\n",
    "geo_layer.add_to(m)\n",
    "                \n",
    "FitZoomBounds(m, bia_gdf['Latitude'], bia_gdf['Longitude'], 1e-6)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the extended bounding box, we can fill it with uniformly distributed points. These points will be used later in the _k_-Center Clustering algorithm. Below is a plot of the bounding box, the BIA's inside it, and the uniformly distributed points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# keep xy proportions consistent\n",
    "scale_factor = (se[0] - sw[0]) / (nw[1] - sw[1])\n",
    "\n",
    "# plot the bounding box\n",
    "plt.figure(figsize=(10 * scale_factor,10))\n",
    "plt.plot(*boundary.exterior.xy, color='black', alpha=0.2);\n",
    "\n",
    "# get the polygons that are fully contained inside the bounding box, inclusive of boundaries\n",
    "polygons = []\n",
    "bia_names = []\n",
    "for bia, poly in zip(bia_gdf['BIA'], bia_gdf['geometry'].values):\n",
    "    if boundary.contains(poly) or poly.crosses(boundary):\n",
    "        polygons.append(poly)\n",
    "        bia_names.append(bia)\n",
    "        \n",
    "        for geom in poly.geoms:\n",
    "            plt.plot(*geom.exterior.xy)\n",
    "\n",
    "#for geom in bia_gdf['geometry'].values:\n",
    "#    plt.plot(*poly.exterior.xy)\n",
    "\n",
    "# get uniformly distributed points inside the bounding box\n",
    "U = GetUnifRect(BBOX, 100000)\n",
    "plt.scatter(U[:1, :], U[1:2, :], color='black', s=1, alpha=0.2)\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points created above need to be converted into shapely.Point objects and stored in a geopandas.GeoSeries in order to use them in the _k_-Center Clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ids = range(U.shape[1])\n",
    "\n",
    "points = []\n",
    "for i in range(U.shape[1]):\n",
    "    u = U[:, i]\n",
    "    \n",
    "    points.append(Point(u[0], u[1]))\n",
    "    \n",
    "points_gs = geopandas.GeoSeries(points, name='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The _k_-Center Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since clustering algorithms require a distance metric, I decided to us the Haversine distance. The Haversine distance is the great circle distance in metres between two points given their latitude and longitude values. This allows for a more accurate distance metric given that it takes into account Earth's curvature, and it allows me to pass a radius in metre units to the clustering algorithm. The function was adapted from an answer by _derricw_ to a [question](https://stackoverflow.com/questions/29545704/fast-haversine-approximation-python-pandas) on Stack Overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got this function from https://stackoverflow.com/questions/29545704/fast-haversine-approximation-python-pandas\n",
    "def HaversineVect(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.    \n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = np.absolute(lon2 - lon1)\n",
    "    dlat = np.absolute(lat2 - lat1)\n",
    "    \n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    m = 6367 * 1000 * c\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is my implementation of the _k_-Center Clustering algorithm from section 4.2 of the paper [Geometric Approximation Algorithms](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.9927&rep=rep1&type=pdf) by Sariel Har-Peled of the University of Illinois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedyKCenter(points, dist_metric='euclidean', n_clusters=None, radius=None):\n",
    "    # Function to perform k-Center Clustering on a set of points.\n",
    "    #\n",
    "    # Args:\n",
    "    #   points: geopandas.GeoSeries of shapely.geometry.Point objects\n",
    "    #   dist_metric: the distance metric to use, must be one of 'euclidean', 'haversine'\n",
    "    #   n_clusters: int, the number of clusters to generate\n",
    "    #   radius: float, the radius of each cluster in meters\n",
    "    #\n",
    "    # Requirements:\n",
    "    #   - if using euclidean distance, the number of clusters must be specified\n",
    "    #   - at least one of n_clusters or radius must be speficied\n",
    "    #\n",
    "    # Returns:\n",
    "    #   centers: list of shapely.geometry.Point objects that represent the centers of the clusters\n",
    "    #   radii: list of float values that represent the radii of the clusters at each iteration\n",
    "    \n",
    "    if not dist_metric in ['euclidean', 'haversine']:\n",
    "        warnings.warn(\"Invalid dist_metric value. Must be one of 'euclidean', 'haversine'. Defaulting to 'euclidean'\")\n",
    "        dist_metric = 'euclidean'\n",
    "\n",
    "    if dist_metric == 'euclidean' and n_clusters == None:\n",
    "        ValueError(\"If using Euclidean distance, you must specify the number of clusters.\")\n",
    "        \n",
    "    if n_clusters == None and radius == None:\n",
    "        ValueError(\"n_clusters and radius are both None. You must specify tha value of at least one.\")\n",
    "        \n",
    "    if n_clusters != None and radius != None:\n",
    "        warnings.warn('Both n_clusters and radius were specified when only one should be. Defaulting to using n_clusters instead of radius.')\n",
    "        radius = None\n",
    "        \n",
    "    if dist_metric == 'euclidean':\n",
    "        # the algorithm starts by choosing a random point as the first center\n",
    "        centers = [random.choice(points.tolist())]\n",
    "\n",
    "        # we want the distance between the center and each point in order to choose the next center\n",
    "        dist = points.distance(centers[0])\n",
    "        radius = []\n",
    "\n",
    "        for n in range(n_clusters):\n",
    "            # we want the minimum distance of each point with all the centers\n",
    "            dist = np.minimum(dist, points.distance(centers[len(centers)-1]))\n",
    "\n",
    "            # new radius and index of the new center\n",
    "            max_dist, argmax_dist = max(dist), np.argmax(dist)\n",
    "\n",
    "            radius.append(max_dist)\n",
    "\n",
    "            centers.append(points[argmax_dist])\n",
    "            \n",
    "        centers = geopandas.GeoSeries(centers)\n",
    "\n",
    "        return centers, radius\n",
    "    \n",
    "    if dist_metric == 'haversine':\n",
    "        # same as above but using the Haversine formula for distance instead\n",
    "        def HaversineHelper(centers, radii, dist):            \n",
    "            center = centers[len(centers)-1]\n",
    "            \n",
    "            dist = np.minimum(dist,\n",
    "                              HaversineVect(longitudes, latitudes, center.x, center.y))\n",
    "\n",
    "            max_dist, argmax_dist = max(dist), np.argmax(dist)\n",
    "\n",
    "            radii.append(max_dist)\n",
    "\n",
    "            centers.append(points[argmax_dist])\n",
    "            \n",
    "            return centers, radii, dist\n",
    "        \n",
    "        centers = [random.choice(points.tolist())]\n",
    "        radii = [10e5]\n",
    "\n",
    "        longitudes = points.x.values\n",
    "        latitudes = points.y.values\n",
    "        \n",
    "        dist = HaversineVect(longitudes, latitudes, centers[0].x, centers[0].y)\n",
    "        \n",
    "        if n_clusters != None:\n",
    "            for n in range(n_clusters):\n",
    "                centers, radii, dist = HaversineHelper(centers, radii, dist)\n",
    "        else:\n",
    "            while min(radii) > radius:\n",
    "                centers, radii, dist = HaversineHelper(centers, radii, dist)\n",
    "                \n",
    "        centers = geopandas.GeoSeries(centers)\n",
    "                \n",
    "        return centers, radii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided that each circle will have an area given by a radius of 50 metres. This is enough to capture most if not all of the venues inside the given area. Furthermore, venues that aren't captured by one API call might get captured by the API call of an adjacent, overlapping circle. Below, I run the algorithm and plot the _Trinity-Bellwoods_ nighbourhood with its corresponding circles to give you an idea of what they look like. An API call will be performed for each one of these circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 110\n",
    "centers = []\n",
    "\n",
    "centers, _ = GreedyKCenter(points_gs, 'haversine', radius=radius)\n",
    "\n",
    "num_centers = len(centers)\n",
    "\n",
    "print('Number of cluster centroids:', num_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = nbh_gdf.loc[nbh_gdf['Neighbourhood'] == 'Trinity-Bellwoods']['geometry'].values[0]\n",
    "contains = np.vectorize(lambda p: poly.contains(p))\n",
    "poly_centers = centers[contains(centers)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "    \n",
    "plt.plot(*poly.exterior.xy)\n",
    "\n",
    "for center in poly_centers:\n",
    "    ax.add_artist(plt.Circle((center.x, center.y), 0.00068, color='r', fill=False))\n",
    "    \n",
    "plt.scatter(poly_centers.x, poly_centers.y, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass a list of category Id's (https://developer.foursquare.com/docs/build-with-foursquare/categories/) to the API call to return businesses in categories of our choice. If not category list is passed to the API call, it will search for businesses of all types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"params.json\", \"r+\") as params_file:\n",
    "    data = json.load(params_file)\n",
    "    \n",
    "    client_id = data['api']['client_id']\n",
    "    client_secret = data['api']['client_secret']\n",
    "    version = data['api']['version']\n",
    "    \n",
    "    url = \"https://api.foursquare.com/v2/venues/categories?client_id={}&client_secret={}&v={}\".format(\n",
    "        client_id, client_secret, version)\n",
    "\n",
    "    tries = 3\n",
    "    success = False\n",
    "    while tries != 0 and not success:\n",
    "        try:\n",
    "            categories = requests.get(url).json()['response']['categories']\n",
    "        except:\n",
    "            tries -= 1\n",
    "            pass\n",
    "        else:\n",
    "            success = True\n",
    "            \n",
    "    if not success:\n",
    "        raise Exception('Tried retrieving data from Foursquare three times without success.')\n",
    "\n",
    "    category_list = [\n",
    "        'Arts & Entertainment',\n",
    "        'Food',\n",
    "        'Nightlife Spot',\n",
    "        'Outdoors & Recreation',\n",
    "        'Professional & Other Places',\n",
    "        'Shop & Service'\n",
    "    ]\n",
    "\n",
    "    category_ids = []\n",
    "    category_names = []\n",
    "    for category in categories:\n",
    "        sub_categories = category['categories']\n",
    "\n",
    "        if category['name'] in category_list:\n",
    "            category_ids.append(str(category['id']))\n",
    "            category_names.append(str(category['name']))\n",
    "            continue\n",
    "            \n",
    "        sub_category_ids = [str(sc['id']) for sc in sub_categories if sc['name'] in category_list]\n",
    "        \n",
    "        sub_category_names = [str(sc['name']) for sc in sub_categories if sc['name'] in category_list]\n",
    "\n",
    "        category_ids.extend(sub_category_ids)\n",
    "        category_names.extend(sub_category_names)\n",
    "\n",
    "    category_info = [{'name': n, 'id': i} for n, i in zip(category_names, category_ids)]\n",
    "    \n",
    "    data['api']['categories'] = category_info\n",
    "    \n",
    "    params_file.seek(0)\n",
    "    json.dump(data, params_file, indent = 4)\n",
    "    params_file.truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from parallelvenues import GetVenuesForNbh\n",
    "\n",
    "def main():\n",
    "    with futures.ProcessPoolExecutor(max_processes) as pool:\n",
    "        venues_gdf = pd.concat(\n",
    "            pool.map(\n",
    "                GetVenuesForNbh,\n",
    "                centers_lat_chunk,\n",
    "                centers_lng_chunk,\n",
    "                radius_chunk,\n",
    "                itertools.repeat(max_threads)\n",
    "            ),\n",
    "            ignore_index=True\n",
    "        )\n",
    "    \n",
    "    return venues_gdf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    max_processes = 5\n",
    "    max_threads = 5\n",
    "    chunksize = 950\n",
    "    center_chunksize = 4500\n",
    "    file_count = 1\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(0, num_centers, center_chunksize):\n",
    "        centers_sub = centers[i: i+center_chunksize]\n",
    "        \n",
    "        # break into chunks that will be passed to the workers\n",
    "        centers_lat_chunk = [centers_sub.y[i: i+chunksize].values for i in range(0, len(centers_sub), chunksize)]\n",
    "        centers_lng_chunk = [centers_sub.x[i: i+chunksize].values for i in range(0, len(centers_sub), chunksize)]\n",
    "        radius_chunk = [radius] * len(centers_lat_chunk)\n",
    "\n",
    "        venues_gdf = main()\n",
    "        \n",
    "        file_name = 'venues_toronto_' + str(file_count) + '.geojson'\n",
    "        venues_gdf.to_file(file_name, drive='GeoJSON', encoding='utf-8')\n",
    "        file_count += 1\n",
    "        \n",
    "        print('Number of venues read in for', len(centers_sub), 'centres:', venues_gdf.shape[0], '\\n')        \n",
    "        del venues_gdf\n",
    "        \n",
    "        time.sleep(4100)\n",
    "\n",
    "    end = time.time()\n",
    "    print('Total run time:', end - start, '\\n')\n",
    "\n",
    "print(venues_gdf.shape)\n",
    "venues_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallelvenues import GetVenuesForNbh\n",
    "\n",
    "def main():\n",
    "    # break into chunks that will be passed to the workers\n",
    "    centers_lat_chunk = [centers.y[i: i+chunksize].values for i in range(0, len(centers), chunksize)]\n",
    "    centers_lng_chunk = [centers.x[i: i+chunksize].values for i in range(0, len(centers), chunksize)]\n",
    "    radius_chunk = [radius] * len(centers_lat_chunk)\n",
    "\n",
    "    with futures.ProcessPoolExecutor(max_processes) as pool:\n",
    "        venues_gdf = pd.concat(\n",
    "            pool.map(\n",
    "                GetVenuesForNbh,\n",
    "                centers_lat_chunk,\n",
    "                centers_lng_chunk,\n",
    "                radius_chunk,\n",
    "                itertools.repeat(max_threads)\n",
    "            ),\n",
    "            ignore_index=True\n",
    "        )\n",
    "    \n",
    "    return venues_gdf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    max_processes = 5\n",
    "    max_threads = 5\n",
    "    chunksize = 900\n",
    "\n",
    "    start = time.time()\n",
    "    venues_gdf = main()\n",
    "    end = time.time()\n",
    "\n",
    "    print('Total run time:', end - start, '\\n')\n",
    "\n",
    "print('Number of venues retrieved:', venues_gdf.shape[0])\n",
    "venues_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the servers at Foursquare are experiencing problems or they go under maintenance and the API returns an error message. We'll remove these rows before doing any data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = venues_gdf.shape[0]\n",
    "venues_gdf = venues_gdf.loc[~(venues_gdf['geometry'].isnull()) | (venues_gdf['Venue ID'].isnull())]\n",
    "print('Number of rows removed due to server errors:', n_rows - venues_gdf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#venues_gdf.to_file('venues_toronto_full.geojson', drive='GeoJSON', encoding='utf-8')\n",
    "venues_gdf.to_file('venues_toronto.geojson', drive='GeoJSON', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "venues_gdf = geopandas.read_file('venues_toronto.geojson')\n",
    "venues_gdf.columns = ['Venue ID', 'Venue', 'Venue Category', 'geometry']\n",
    "venues_gdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Venue Data Exploring and Cleaning\n",
    "Now that we have retrieved venues, we'll look at the data to see if it needs cleaning. First, let's check if there are any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(venues_gdf.isnull(), cbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is missing data on two of the columns, and it seems that the missing rows align between the two columns. Let's see how many values are missing and then drill into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_venues_gdf = venues_gdf.loc[(venues_gdf['Venue ID'].isnull()) & (venues_gdf['geometry'].isnull())]\n",
    "\n",
    "print('Number of values missing in \"Venue ID\":', venues_gdf.loc[venues_gdf['Venue ID'].isnull()].shape[0])\n",
    "print('Number of values missing in \"geometry\":', venues_gdf.loc[venues_gdf['geometry'].isnull()].shape[0])\n",
    "print('Number of values missing in both columns at the same time:', missing_venues_gdf.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are multiple rows where both columns have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_gdf.loc[(venues_gdf['Venue ID'].isnull()) & (venues_gdf['geometry'].isnull())].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When both 'Venue ID' and 'geometry' are missing, it means that the API didn't return any venues for that specific area (you can see the coordinate values in the other two columns). My intuition is that these are big green spaces (Toronto has a lot of parks) so that is why no venue was found in those areas. But let's map them, as well as the busiest neighbourhoods, to make sure that is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_riverdale_traffic = geopandas.GeoDataFrame(traffic_df.loc[traffic_df['Neighbourhood'] == 'North Riverdale'])\n",
    "\n",
    "m = CreateMap(centre_lat, centre_lng, zoom_start=12, scrollWheelZoom=False)\n",
    "\n",
    "geo_layer = AddPolygonLayer(m, busiest_nbh, return_layer=True)\n",
    "\n",
    "folium.GeoJsonTooltip(['Neighbourhood']).add_to(geo_layer)\n",
    "geo_layer.add_to(m)\n",
    "\n",
    "AddCircleMarkers(m, missing_venues_gdf['Venue'], missing_venues_gdf['Venue Category'], missing_venues_gdf['geometry'],\n",
    "                 radius=3, color='black', fillColor='black', fill=True, weight=1)\n",
    "\n",
    "AddCircleMarkers(m, n_riverdale_traffic.geometry.y, n_riverdale_traffic.geometry.x, n_riverdale_traffic['Intersection'],\n",
    "                 radius=3, color='red', fillColor='red', fill=True)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a lot of areas with no venues fall inside big green spaces but also in areas covered in water. Furthermore, there are other areas that don't have venues around them and we can see that they fall outside the busiest neighbourhoods in the city. This makes sense since the further away you move from the core of the city, the more residential neighbourhoods there are and thus the density of venues decreases. The only neighbourhood on the map that has a significant number of areas without venues is North Riverdale at the east end, so I decided to plot the intersections in our dataset for pedestrian volume (red circles). We see that the intersections with pedestrian volume data lie on the edges of the neighbourhood, which can explain why it is a busy neighbourhood yet it doesn't have many venues in the middle.\n",
    "<br><br>\n",
    "The map gives a good idea of where venues are concentrated in the city, and it also indicates that the pedestrian volume and venue density are positively correlated. It is worth exploring the correlation of venue density and pedestrian volume later on.\n",
    "<br><br>\n",
    "Given that the API didn't return venues for residential areas, parks, and the lake we can get rid of these missing values and concentrate on the venues that were retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_gdf = venues_gdf[~((venues_gdf['Venue ID'].isnull()) & (venues_gdf['geometry'].isnull()))]\n",
    "print('Number of venues remaining after removing missing data:', venues_gdf.shape[0])\n",
    "sns.heatmap(venues_gdf.isnull(), cbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed the rows with missing data, let's see if we got any duplicates by looking at the percentage of unique values of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_gdf.nunique() / venues_gdf.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ok for the columns 'Venue', 'Venue Category', and 'geometry' to have duplicate values because the same business can have multiple locations, different businesses can belong to the same category, and multiple businesses can be located in the same building, respectively. However, the venue ID's should be unique since businesses ara given unique identifiers even if they belong to the same franchise. Let's look at the duplicate venue ID's and see if they have the same values for all columns (true duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate venue ID's are true duplicates:\", venues_gdf.drop_duplicates().shape[0] == venues_gdf['Venue ID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows with duplicate venue ID's are true duplicates, i.e. all the rows contain the same values for each column. Therefore, we'll remove the duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_gdf.drop_duplicates(inplace=True)\n",
    "venues_gdf.reset_index(drop=True, inplace=True)\n",
    "print('Number of venues left after removing duplicates:', venues_gdf.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned up the data, let's look at the distribution of venue categories. First, we'll see how many unique venue categories there are to determine if we need to amalgamate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique venue categories:', venues_gdf['Venue Category'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a large number of unique venue categories which means the data is too granular. When the API returns a venue, the category of the venue can be a sub-category of a different category. In order to reduce the granularity of the data, we can look at categories that fall under a given percentile and replace them with their parent category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceVenueCategory(df, col_name, old_categories, new_category):\n",
    "    # Function to replace the category of a venue with a new category.\n",
    "    #\n",
    "    # Args:\n",
    "    #   df: DataFrame containing venue information; must have a column names 'Venue Category'\n",
    "    #   col_name: string, the name of the column in df that contains venue categories\n",
    "    #   old_categories: list of old categories to be replaced.\n",
    "    #   new_category: name of the new category.\n",
    "    #\n",
    "    # Returns: None\n",
    "    \n",
    "    if len(old_categories) == 0:\n",
    "        warnings.warn('No categories to replace were given.')\n",
    "        \n",
    "    if df.loc[np.in1d(df[col_name], old_categories), col_name].shape[0] == 0:\n",
    "        warnings.warn(\"The old categories don't exist in the DataFrame. Nothing will be replaced.\")\n",
    "    \n",
    "    try:\n",
    "        df.loc[np.in1d(df[col_name], old_categories), col_name] = new_category\n",
    "    except KeyError:\n",
    "        raise Exception(\"The column '\" + str(col_name) + \"' does not exist in the DataFrame.\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetParentCategory(category_list, p_category, category):\n",
    "    # Function to get the parent category of a given category\n",
    "    #\n",
    "    # Args:\n",
    "    #   category_list: list of nested dictionaries that contain categories and their sub-categories\n",
    "    #   p_category: string, the parent category of the argument 'category'\n",
    "    #   category: string, the category of the venue\n",
    "    #\n",
    "    # Side Effects: the global variable 'parent_category' is modified\n",
    "    \n",
    "    global parent_category\n",
    "    for cat in category_list:\n",
    "        if cat['name'] == category:\n",
    "            parent_category = p_category\n",
    "        elif cat['categories'] == []:\n",
    "            continue\n",
    "        else:\n",
    "            GetParentCategory(cat['categories'], cat['name'], category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetReplacementDict(df, col_name, percentile, categories):\n",
    "    # Creates a dictionary where the keys are the old categories to be replaced with the new categories given by the values.\n",
    "    #\n",
    "    # Args:\n",
    "    #   df: DataFrame or GeoDataFrame that contains venue categories in the column given by 'col_name'\n",
    "    #   col_name: string, the name of the column in df that contains venue categories\n",
    "    #   percentile: float, category counts under this value will be replaced\n",
    "    #   categories: list of nested dictionaries that contain categories and their sub-categories\n",
    "    #\n",
    "    # Side Effects: the global variable 'parent_category' is modified\n",
    "    #\n",
    "    # Returns:\n",
    "    #   category_dict: dict, dictionary where the keys are the old categories and the values are the new ones\n",
    "    \n",
    "    global parent_category\n",
    "    \n",
    "    category_counts = df[col_name].value_counts()\n",
    "    threshold = np.percentile(category_counts, percentile)\n",
    "    category_list = category_counts[category_counts <= threshold]\n",
    "    \n",
    "    category_dict = {}\n",
    "    for category in category_list.index:\n",
    "        GetParentCategory(categories, '', category)\n",
    "\n",
    "        if parent_category != '':\n",
    "            category_dict.update({category: parent_category})\n",
    "\n",
    "            parent_category = ''\n",
    "            \n",
    "    print('Number of categories in replacement dictionary:', len(category_dict))      \n",
    "    return category_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the number of categories that will be replaced, and the new number of unique categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_category = ''\n",
    "category_dict = GetReplacementDict(venues_gdf, 'Venue Category', 80, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in category_dict.items():\n",
    "    ReplaceVenueCategory(venues_gdf, 'Venue Category', k, v)\n",
    "    \n",
    "print('New number of unique categories:', venues_gdf['Venue Category'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next table you can see that there are still some categories with very low counts so they can be replaced with their parent categories. However, we don't want to replace too many since we don't want to generalize the data too much, so we'll use the 10th percentile this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_gdf['Venue Category'].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_category = ''\n",
    "category_dict = GetReplacementDict(venues_gdf, 'Venue Category', 10, categories)\n",
    "\n",
    "for k, v in category_dict.items():\n",
    "    ReplaceVenueCategory(venues_gdf, 'Venue Category', k, v)\n",
    "    \n",
    "print('Number of unique categories:', venues_gdf['Venue Category'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to keep the uppermost parent category of each venue so we can easily visualize the types of venues in each BIA. Furthermore, even when I've specified the categories I want the API returns a few venues not in those categories so I remove them from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = venues_gdf.shape[0]\n",
    "ignore_list = ['College & University', 'Event', 'Residence', 'Travel & Transport']\n",
    "venues_gdf = venues_gdf[~np.in1d(venues_gdf['Venue Category'], ignore_list)]\n",
    "print('Number of venues with unwanted categories remvoed:', n_rows - venues_gdf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_gdf.loc[:, ('Parent Category')] = venues_gdf.loc[:, ('Venue Category')]\n",
    "\n",
    "while not all(c in category_list for c in np.unique(venues_gdf['Parent Category'])):\n",
    "    parent_category = ''\n",
    "    category_dict = GetReplacementDict(venues_gdf, 'Parent Category', 100, categories)\n",
    "    \n",
    "    for k, v in category_dict.items():\n",
    "        ReplaceVenueCategory(venues_gdf, 'Parent Category', k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Venues in Business Improvement Areas\n",
    "Now that we have a solid venue dataset, we would like to explore the types of venues that area in existing BIA's. To do so, we'll assign the corresponding BIA to each venue and drill into each BIA and see the distribution of venue types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bia_gdf.sort_values(by='BIA', inplace=True)\n",
    "\n",
    "venues_bia_df = GetNbhForPoints(\n",
    "    bia_gdf['BIA'],\n",
    "    bia_gdf['geometry'],\n",
    "    venues_gdf['Venue ID'],\n",
    "    venues_gdf['geometry'],\n",
    "    'Venue ID'\n",
    ")\n",
    "\n",
    "venues_bia_df.rename(columns={'Neighbourhood': 'BIA'}, inplace=True)\n",
    "venues_bia_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_gdf = venues_gdf.merge(venues_bia_df.set_index('Venue ID'), how='inner', on='Venue ID')\n",
    "venues_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map below shows the venues that don't fall within a BIA. If you zoom in close enough, you will see that some venues are on or cross the boundaries of the BIA's. The venues mentioned before should be part of the BIA that they cross, so we will assign them to their corresponding BIA given that they are close enough to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_undef_gdf = venues_gdf.loc[venues_gdf['BIA'] == 'Undefined']\n",
    "\n",
    "m = CreateMap(centre_lat, centre_lng, zoom_start=11, scrollWheelZoom=False)\n",
    "\n",
    "geo_layer = AddPolygonLayer(m, bia_gdf, return_layer=True)\n",
    "\n",
    "folium.GeoJsonTooltip(['BIA']).add_to(geo_layer)\n",
    "geo_layer.add_to(m)\n",
    "\n",
    "AddCircleMarkers(m, venues_undef_gdf['geometry'].y, venues_undef_gdf['geometry'].x,\n",
    "                 venues_undef_gdf.index, radius=1, color='black', fillColor='black', fill=True,\n",
    "                 weight=1)\n",
    "\n",
    "FitZoomBounds(m, bia_gdf['Latitude'], bia_gdf['Longitude'], 1e-6)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_rows_old = venues_undef_gdf.shape[0]\n",
    "\n",
    "ReplaceUndefinedPoly(venues_gdf, bia_gdf, 'BIA', max_dist=0.0001)\n",
    "venues_undef_gdf = venues_gdf.loc[venues_gdf['BIA'] == 'Undefined']\n",
    "\n",
    "n_rows_new = venues_undef_gdf.shape[0]\n",
    "print('Number of venues that got a BIA assigned to them:', n_rows_old - n_rows_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of venues by parent category inside and outside of BIA's. Below is a plot of the number of venues by parent category inside and outside BIA's. If you hover over the plot, you can see information for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_gdf.loc[:, ('IsInBIA')] = pd.Series(np.where(venues_gdf['BIA'] == 'Undefined', 0, 1), venues_gdf.index)\n",
    "\n",
    "bia_cat_count = venues_gdf.groupby(['IsInBIA', 'Parent Category'])['Parent Category'].agg([('Parent Count', 'count')]).reset_index()\n",
    "bia_cat_count.loc[:, 'IsInBIA'] = pd.Series(np.where(bia_cat_count['IsInBIA'] == 1, 'Inside BIA', 'Outside BIA'), bia_cat_count.index)\n",
    "bia_cat_count.loc[:, ('Parent Count Pct')] = bia_cat_count['Parent Count'].apply(lambda x: np.round(100 * x / np.sum(bia_cat_count['Parent Count']), 2))\n",
    "bia_cat_count.sort_values('IsInBIA', inplace=True)\n",
    "\n",
    "fig = px.bar(bia_cat_count, x='IsInBIA', y='Parent Count', color='Parent Category', barmode='group')\n",
    "fig.update_layout(\n",
    "    title=\"Number of Venues Inside and Outside BIA's by Parent Category\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title='Inside/Outside BIA',\n",
    "    yaxis_title='Number of Venues'\n",
    ")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_diff_bia_counts = np.round(100 * (venues_gdf.shape[0] - 2*venues_gdf['IsInBIA'].sum()) / (venues_gdf.shape[0] - venues_gdf['IsInBIA'].sum()), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "pct_diff_bia_counts": {}
    }
   },
   "source": [
    "In the plot above we see some interesting differences in the number of venues inside and outside BIA's. First of all, it is worth noting that there are {{pct_diff_bia_counts}}% more venues outside of BIA's than inside so we have to keep this in mind when comparing the values above. There is a similar number of 'Shop & Service' venues inside and outside BIA's; but, since there is a higher number of venues outside the BIA's we can conclude that when designating BIA's people are interested in including these types of venues. Similary, the venues in the 'Nightlife Spot' have a similar amount of venues inside and outside BIA's. The venues in the 'Food' category are also of high interest as shown by the fact that there are more food venues inside BIA's than outside BIA's. It is interesting to see that even though BIA's cover a lower percentage of our area of interest, there are more food venues inside them than outside of them. The rest of the venues don't differ much, except for those in the 'Outdoors & Recreation' category. However, given that BIA's cover a lower percentage of land in addition to most green spaces being outside BIA's, it is expected to see this difference in the number of venues.\n",
    "<br><br>\n",
    "People love to go out shopping and eating so it makes sense that BIA's contain a large proportion of venues that meet visitor's interests. This aligns with one of the city's goals when designating BIA's, which is to attract visitors to these areas. Furthermore, if more people are consistently visiting a specific area of the city this would attract other businesses that can take advantage of the pedestrian volume in the BIA. This further aligns with the goal of attracting new businesses to BIA's.\n",
    "<br><br>\n",
    "Our area of interest has 52 BIA's which are very different from each other and were designated based on the venues in those areas. Therefore, we would like to explore the distribution of venues to see if there is a difference in distribution of venues depending on the pedestrian volume of each BIA. In order to do so, we will assign intersections to each BIA and create ranges of pedestrian volume to group BIA's from lowest volume to highest volume and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bia_int_df = GetNbhForPoints(\n",
    "    bia_gdf['BIA'],\n",
    "    bia_gdf['geometry'],\n",
    "    traffic_df['Id'],\n",
    "    traffic_df['geometry'],\n",
    "    'Id',\n",
    "    'BIA'\n",
    ")\n",
    "bia_int_df.loc[:, ('Id')] = pd.to_numeric(bia_int_df.loc[:, ('Id')])\n",
    "bia_int_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df = traffic_df.merge(bia_int_df, how='left', on='Id')\n",
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some intersections are very close to the boundaries of the BIA's so they need to be assigned to their closest BIA. The pedestrian traffic of such intersections would affect the traffic through the BIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = traffic_df.loc[traffic_df['BIA'] != 'Undefined'].shape[0]\n",
    "ReplaceUndefinedPoly(traffic_df, bia_gdf, 'BIA', 0.001)\n",
    "print('Number of intersections assigned to a BIA:', traffic_df.loc[traffic_df['BIA'] != 'Undefined'].shape[0] - n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ped_vol_bia = traffic_df.loc[traffic_df['BIA'] != 'Undefined'].groupby(['BIA'])['PedestrianVolume'].agg([('Total Pedestrian Volume', 'sum')]).reset_index()\n",
    "\n",
    "ped_vol_bia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ped_vol_bia['Pedestrian Volume Level'] = pd.qcut(\n",
    "    ped_vol_bia['Total Pedestrian Volume'],\n",
    "    q=7,\n",
    "    labels=[\n",
    "        'Lowest',\n",
    "        'Very Low',\n",
    "        'Low',\n",
    "        'Medium',\n",
    "        'High',\n",
    "        'Very High',\n",
    "        'Highest']\n",
    ")\n",
    "\n",
    "ped_vol_bia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bia_gdf = bia_gdf.merge(ped_vol_bia, how='inner', on='BIA')\n",
    "bia_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_gdf = venues_gdf.merge(bia_gdf[['BIA', 'Pedestrian Volume Level']], how='left', on='BIA')\n",
    "venues_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_sub_gdf = venues_gdf.loc[~venues_gdf['Pedestrian Volume Level'].isnull()]\n",
    "\n",
    "venues_ped_level = venues_sub_gdf.groupby(['Parent Category', 'Pedestrian Volume Level'])['Parent Category'].agg([('Parent Count', 'count')]).reset_index()\n",
    "\n",
    "ped_level_count = venues_ped_level.groupby(['Pedestrian Volume Level'])['Parent Count'].agg([('Total Count', 'sum')]).reset_index()\n",
    "\n",
    "venues_ped_level = venues_ped_level.loc[\n",
    "    ~np.in1d(\n",
    "        venues_ped_level['Parent Category'],    \n",
    "        ['Arts & Entertainment', 'Nightlife Spot', 'Outdoors & Recreation'])\n",
    "]\n",
    "\n",
    "venues_ped_level = venues_ped_level.merge(ped_level_count, how='inner', on='Pedestrian Volume Level')\n",
    "\n",
    "venues_ped_level.loc[:, ('Parent Percentage')] = 100 * venues_ped_level.loc[:, ('Parent Count')] / venues_ped_level.loc[:, ('Total Count')]\n",
    "\n",
    "fig = px.bar(venues_ped_level, x='Pedestrian Volume Level', y='Parent Percentage', color='Parent Category', barmode='group')\n",
    "fig.update_layout(\n",
    "    title=\"Percentage of Venues Inside BIA's by Pedestrian Volume Level and Parent Category\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title='Pedestrian Volume Level',\n",
    "    yaxis_title='Percentage of Venues'\n",
    ")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCartDist(dist):\n",
    "    # Transforms a distance in metres to city block distance in the cartesian plane using \n",
    "    # the bisection method.\n",
    "    #\n",
    "    # Args:\n",
    "    #   dist: float, distance in metres\n",
    "    #\n",
    "    # Returns:\n",
    "    #   c: float, city block distance in cartesian coordinates\n",
    "    #\n",
    "    # Example:\n",
    "    #   GetCityBlockDist(25) -> 0.000225\n",
    "    \n",
    "    # create a function where the root is the latitude value that results in a Haversine\n",
    "    # distance of dist metres.\n",
    "    f = lambda lat: HaversineVect(0, 0, 0, lat) - dist\n",
    "    dist_mag = int(math.log10(dist))\n",
    "    \n",
    "    # 1e-5 is equivalent to roughly 1 metre distance\n",
    "    a = dist_mag * 1e-5\n",
    "    b = 10**(dist_mag+1) * 1e-5\n",
    "    c = (a + b) / 2\n",
    "    \n",
    "    # bisection method\n",
    "    while abs(f(c)) > 1e-6:\n",
    "        if f(c) < 0:\n",
    "            a = c\n",
    "        else:\n",
    "            b = c\n",
    "            \n",
    "        c = (a + b) / 2\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create clusters of venues from those outside BIA's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "venues_out_gdf = venues_gdf.loc[venues_gdf['IsInBIA'] != 1].reset_index().copy()\n",
    "\n",
    "venues_out_gdf.loc[:, 'Longitude'] = venues_out_gdf.loc[:, 'geometry'].x\n",
    "venues_out_gdf.loc[:, 'Latitude'] = venues_out_gdf.loc[:, 'geometry'].y\n",
    "\n",
    "min_samples = 5\n",
    "\n",
    "features = squareform(pdist(venues_out_gdf[['Longitude', 'Latitude']], metric='cityblock'))\n",
    "X = MinMaxScaler().fit_transform(features)\n",
    "\n",
    "start_time = time.time()\n",
    "nearest_neighbours = NearestNeighbors(n_neighbors=min_samples+1, metric='precomputed')\n",
    "end_time = time.time()\n",
    "print('Total run-time:', end_time-start_time, '\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "neighbours = nearest_neighbours.fit(X)\n",
    "end_time = time.time()\n",
    "print('Total run-time:', end_time-start_time, '\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "distances, indices = neighbours.kneighbors(X)\n",
    "end_time = time.time()\n",
    "print('Total run-time:', end_time-start_time, '\\n')\n",
    "\n",
    "distances = np.sort(distances[:, min_samples], axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "print(distances[knee.knee])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the maximum cityblock distance in metres that two venues have to be from each other for one \n",
    "# to be considered as in the neighbourhood of the other\n",
    "dist = 500\n",
    "cart_dist = GetCartDist(dist)\n",
    "\n",
    "db = DBSCAN(eps=cart_dist, min_samples=min_samples, metric='precomputed', n_jobs=-1).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = np.unique(db.labels_[db.labels_ != -1])\n",
    "\n",
    "venues_out_gdf['Cluster Label'] = db.labels_\n",
    "venues_plot_gdf = venues_out_gdf.loc[venues_out_gdf['Cluster Label'] != - 1].reset_index(drop=True)\n",
    "\n",
    "cmap = plt.cm.get_cmap('gist_rainbow')\n",
    "color_matrix = cmap(np.arange(cmap.N))\n",
    "color_idx = np.round(np.linspace(0, color_matrix.shape[0] - 1, venues_plot_gdf['Cluster Label'].nunique())).astype(int)\n",
    "\n",
    "color_array = np.apply_along_axis(colors.rgb2hex, 1, color_matrix[color_idx])\n",
    "random.shuffle(color_array)\n",
    "\n",
    "m = CreateMap(centre_lat, centre_lng, zoom_start=11, scrollWheelZoom=False)\n",
    "\n",
    "geo_layer = AddPolygonLayer(m, bia_gdf, return_layer=True)\n",
    "\n",
    "folium.GeoJsonTooltip(['BIA']).add_to(geo_layer)\n",
    "geo_layer.add_to(m)\n",
    "\n",
    "for i, cluster_label in enumerate(cluster_labels):\n",
    "    venues_plot_cluster = venues_plot_gdf.loc[venues_plot_gdf['Cluster Label'] == cluster_label].reset_index()\n",
    "    cluster_color = color_array[i]\n",
    "    \n",
    "    AddCircleMarkers(m, venues_plot_cluster['geometry'].y, venues_plot_cluster['geometry'].x,\n",
    "                     [cluster_label] * venues_plot_cluster.shape[0], radius=5, fill=True, color=cluster_color, \n",
    "                     fillColor=cluster_color, weight=2)\n",
    "\n",
    "FitZoomBounds(m, bia_gdf['Latitude'], bia_gdf['Longitude'], 1e-6)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_count = venues_gdf.groupby('BIA')['Venue'].agg(Count='count')\n",
    "min_bia_count = venue_count['Count'].min()\n",
    "print(\"Number of venues in the BIA with the least amount of venues:\", min_bia_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = venues_out_gdf['Cluster Label'].value_counts()\n",
    "potential_bia = cluster_count[cluster_count >= 35].index.values\n",
    "venues_out_gdf = venues_out_gdf.loc[\n",
    "    (venues_out_gdf['Cluster Label'] != -1) & \n",
    "    (np.in1d(venues_out_gdf['Cluster Label'], potential_bia))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('gist_rainbow')\n",
    "color_matrix = cmap(np.arange(cmap.N))\n",
    "color_idx = np.round(np.linspace(0, color_matrix.shape[0] - 1, venues_plot_gdf['Cluster Label'].nunique())).astype(int)\n",
    "\n",
    "color_array = np.apply_along_axis(colors.rgb2hex, 1, color_matrix[color_idx])\n",
    "random.shuffle(color_array)\n",
    "\n",
    "m = CreateMap(centre_lat, centre_lng, zoom_start=11, scrollWheelZoom=False)\n",
    "\n",
    "geo_layer = AddPolygonLayer(m, bia_gdf, return_layer=True)\n",
    "\n",
    "folium.GeoJsonTooltip(['BIA']).add_to(geo_layer)\n",
    "geo_layer.add_to(m)\n",
    "\n",
    "for i, cluster_label in enumerate(cluster_labels):\n",
    "    venues_plot_cluster = venues_out_gdf.loc[venues_out_gdf['Cluster Label'] == cluster_label].reset_index()\n",
    "    cluster_color = color_array[i]\n",
    "    \n",
    "    AddCircleMarkers(m, venues_plot_cluster['geometry'].y, venues_plot_cluster['geometry'].x,\n",
    "                     [cluster_label] * venues_plot_cluster.shape[0], radius=5, fill=True, color=cluster_color, \n",
    "                     fillColor=cluster_color, weight=2)\n",
    "\n",
    "FitZoomBounds(m, bia_gdf['Latitude'], bia_gdf['Longitude'], 1e-6)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bia_gdf.loc[:, 'Log Total Pedestrian Volume'] = np.log(bia_gdf.loc[:, 'Total Pedestrian Volume'])\n",
    "fig = px.histogram(bia_gdf, x=\"Log Total Pedestrian Volume\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {'Cluster Label': [], 'geometry': []}\n",
    "cluster_labels = np.unique(venues_out_gdf['Cluster Label'])\n",
    "\n",
    "for label in cluster_labels:\n",
    "    poly = MultiPoint(venues_out_gdf.loc[venues_out_gdf['Cluster Label'] == label].geometry.reset_index(drop=True)).convex_hull\n",
    "    \n",
    "    cluster_dict['Cluster Label'].append(label)\n",
    "    cluster_dict['geometry'].append(poly)\n",
    "    \n",
    "cluster_gdf = geopandas.GeoDataFrame(pd.DataFrame.from_dict(cluster_dict))\n",
    "\n",
    "cluster_vol_df = GetNbhForPoints(\n",
    "    cluster_labels,\n",
    "    cluster_gdf.geometry,\n",
    "    traffic_df['Id'],\n",
    "    traffic_df['geometry'],\n",
    "    col_name = 'Cluster Label'\n",
    ")\n",
    "cluster_vol_df.loc[:, 'Id'] = cluster_vol_df.loc[:, 'Id'].astype('int64')\n",
    "\n",
    "traffic_df = traffic_df.merge(cluster_vol_df, how='inner', on='Id')\n",
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = traffic_df.loc[traffic_df['Cluster Label'] == 'Undefined'].shape[0]\n",
    "ReplaceUndefinedPoly(traffic_df, cluster_gdf, 'Cluster Label', max_dist=0.001)\n",
    "print('Number of intersections assigned to clusters:', n_rows - traffic_df.loc[traffic_df['Cluster Label'] == 'Undefined'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ped_vol = traffic_df.groupby('Cluster Label')['PedestrianVolume'].agg([('Total Pedestrian Volume', 'sum')]).reset_index()\n",
    "cluster_gdf = cluster_gdf.merge(cluster_ped_vol, how='inner', on='Cluster Label')\n",
    "cluster_gdf.loc[:, 'Log Total Pedestrian Volume'] = np.log(cluster_gdf.loc[:, 'Total Pedestrian Volume'])\n",
    "cluster_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(cluster_gdf, 'Log Total Pedestrian Volume')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bia_grouped = venues_gdf.groupby(['BIA', 'Parent Category'])\n",
    "parent_count = bia_grouped['Venue'].agg([('Parent Category Count', 'count')])\n",
    "values = bia_grouped['BIA'].agg('count')\n",
    "pc_df = parent_count['Parent Category Count'].apply(lambda x: pd.Series(x)).unstack()\n",
    "pc_df = pc_df.apply(lambda x: x / np.sum(x) * 100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df.columns = pc_df.columns.set_levels(['Percentage of Venues',], level=0)\n",
    "pc_df.fillna(0, inplace=True)\n",
    "pc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bia_gdf.columns = pd.MultiIndex.from_product([bia_gdf.columns, ['']])\n",
    "bia_gdf = bia_gdf.merge(pc_df, how='inner', on='BIA')\n",
    "bia_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "features = bia_gdf[['Log Total Pedestrian Volume', 'Percentage of Venues']]\n",
    "\n",
    "scaler = MinMaxScaler().fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
